---
title: "Weather Pred Assessment"
format: html
editor: visual
---

-   update: get map of MA by zip code geometries

-   find centroids of each zip code

-   pull current and predicted weather for each

-   Visual of US for current weather

-   Visual of US for future weather

-   Placed onto website

Trial2.qmd: accessing Tomorrow's API Trial3.qmd: accessing NOAA's API https://www.ncdc.noaa.gov/cdo-web/webservices/v2#gettingStarted

Main idea: correlate historical weather with some data Then use weather predictions to estimate future XX - temp in each zip code, multiplied by \# people in each zip code Maybe voting records? Even if looked at all counties for a certain year, would also need to correct for temp, political leanings, etc. Maybe energy usage if I can get it on a local scale Maybe traffic stops per town - use case: help police departments estimate days of max revenue? Maybe stock prices of energy dependent companies (Generac?) Maybe commodity prices --\> how do we get average temp for each country? - would have to multiply temp by people within each grid cell - Frequency of crime? - Frequency of web searches for item? Number of website visits / uses? - could be used for company to plan computer usage - maybe avg social media users: - crime incidents report may be the most important - can associate specific temps/humidities/windspeeds with crime locations on day/time - Or could aggregate by neighborhood/zip code / whole city that day, get avg temp for each type of crime - just downloaded 2022 as tmpdfeo3qy2.csv

-   are any crude oil / energy prices based on US consumption only? If so, could def do something here
-   otherwise, crime may be most interesting open data daily I could work with

```{python Initialize libraries}
import json
import requests
import pandas as pd
from datetime import datetime
import psycopg2
import datetime as dt
import pytz
import requests
import matplotlib.pyplot as plt
from sqlalchemy import create_engine
from noaa_sdk import NOAA
import subprocess
```

## Framework:

1.  Pull in forecasted weather on regular basis and store it in PostgreSQL DB
    1.  likely by the hour
2.  Gather realtime / historical data for some location, for those same timeframes
3.  Compare them to assess accuracy of forecasted predictions

## Pull in forecasted weather on regular basis and store it directly in PostgreSQL DB

```{python}
url = "https://api.tomorrow.io/v4/timelines?location=42.3478%2C%20-71.0466&fields=temperature&fields=&units=metric&timesteps=1h&startTime=now&endTime=nowPlus48h&apikey=WgX3pBOikmlLo5MDTzfdHPaYuSqyhuMZ"
url = "https://api.tomorrow.io/v4/timelines?location=42.3478%2C%20-71.0466&fields=temperature&fields=&units=metric&timesteps=1d&startTime=now&endTime=nowPlus5d&apikey=WgX3pBOikmlLo5MDTzfdHPaYuSqyhuMZ"
url = "https://api.weather.gov/points/39.7456,-97.0892"
url="https://www.ncei.noaa.gov/cdo-web/api/v2/{endpoint}"
url="https://www.ncei.noaa.gov/cdo-web/api/v2/data?datasetid=PRECIP_15&stationid=COOP:010008&units=metric&startdate=2010-05-01&enddate=2010-05-31

headers = {
    "accept": "application/json",
    "Accept-Encoding": "gzip"
}

headers = {
    "accept": "application/json",
    "Accept-Encoding": "gzip",
    "token": "xnrkMjrkPGDuwrljOUbKlfqjkMVKAtJe"
}
headers = {
    "accept": "application/json",
    "Accept-Encoding": "gzip",
    "token": "xnrkMjrkPGDuwrljOUbKlfqjkMVKAtJe"
}
response = requests.get(url, headers=headers)
print(response.text)
```

```{python Use wrapper to get NOAA API data}
n = NOAA()
  res = n.get_forecasts('02129', 'US')
  for i in res:
      print(i)

#n = NOAA()
  #n.points_forecast(40.7314, -73.8656, type='forecastGridData')
```

```{python Use wrapper to get recent NOAA Historical data}
n = NOAA()
observations = n.get_observations('11365','US',start='2023-07-01',end='2023-07-05')
for observation in observations:
    print(observation)
    break
```

```{python get current date and time for saving forecasted data}
# # datetime object containing current date and time
# now = datetime.now()
# # dt_string = now.strftime("%Y%m%d_%H:%M:%S")
# dt_string = now.strftime("%Y%m%d")
# print("date and time =", dt_string)
# table_name="date_"+dt_string
```

## Storing json in my DB

```{python }
#json_to_DB(response_string, NOAAcast1)

#def json_to_DB(response_string, table_name, DB="postgres"):

# Convert JSON data to string
json_string = json.dumps(response.json())

# Establish connection to PostgreSQL database
conn = psycopg2.connect(database ="postgres", user = "postgres",
                        password = "Brazilll", host = "localhost", 
                        port = "5432")

# Create a cursor object to execute SQL queries
cur = conn.cursor()

# Create a table in PostgreSQL with a JSON column
cur.execute("CREATE TABLE IF NOT EXISTS NOAAcast1 (id SERIAL PRIMARY KEY, data JSONB)")

# Insert the JSON data into the table
cur.execute("INSERT INTO NOAAcast1 (data) VALUES (%s)", (json_string,))
# cur.execute("INSERT INTO "+table_name+" (data) VALUES (%s)", (json_string,))


# Commit the changes to the database
conn.commit()

# Close the cursor and the connection
cur.close()
conn.close()

```

-   Can pause here to check pgadmin 4 for submission
-   Note for later: could store each data retrieval as a single line in a single table of DB, seems to be most efficient storage method
-   all predictions in ten lines of

## Retrieving that same data from DB into workspace

```{python }
def db_to_workspace_string(table_name, sql_arg="SELECT data FROM ", DB="postgres"):

  # Attention: this works with NOAA string, but not Tomorrow's
  # Establish connection to PostgreSQL database
  conn = psycopg2.connect(database =DB, user = "postgres",
                          password = "Brazilll", host = "localhost", 
                          port = "5432")
  
  # Create a cursor object to execute SQL queries
  cur = conn.cursor()
  
  # Select the JSON data from the table
  # cur.execute("SELECT data FROM noaacast1")
  cur.execute(sql_arg + table_name)
  
  # Fetch the retrieved data, which should already be stored as a python object
  # json_data = cur.fetchone()[0]
  for row in cur.fetchall():
      string = row[0]
      # Do something with the individual string, such as printing it
      # fxn here to save each string as 
      # print(string) # prob not necessary
  
  # Close the cursor and the connection
  cur.close()
  conn.close()
  
  return(string)

# Print the retrieved JSON data
# print(json_data)

jj = db_to_workspace_string("noaacast1")
```

## Turning it into readable DF

```{python Tomorrow Json string to readable df fxn}

def jsonstring_to_df(json_data_arg):

  # %% turn to Df
  # timelines = parsed_data['data']['timelines']
  timelines = json_data_arg['data']['timelines']
  intervals = []
  for timeline in timelines:
      intervals.extend(timeline['intervals'])
  
  # Create a DataFrame from the extracted data
  data = pd.DataFrame(intervals)
  
  
  # %% Turn Datetimes into columns
  df = pd.DataFrame(data['startTime'])
  
  # Split datetime into separate columns
  df[['date', 'time']] = df['startTime'].str.split('T', expand=True)
  df[['year', 'month', 'day']] = df['date'].str.split('-', expand=True)
  df[['hour', 'minute', 'second']] = df['time'].str[:-1].str.split(':', expand=True)
  
  # Remove unnecessary columns
  df = df.drop(['startTime', 'date', 'time'], axis=1)
  
  # %% Pull the column of temperature values
  df['temp'] = data['values'].apply(lambda x: x['temperature'])
      # access values in 'values' column
      # lambda fxn: extracts value, temp, from each key, x
      
  return(df)

# df2 = jsonstring_to_df(json_data)
df2 = jsonstring_to_df(string) # Attention: this works
```

```{python Tomorrow Json string to readable df fxn}

def jsonstring_to_df(json_data_arg):

  # %% turn to Df
  # timelines = parsed_data['data']['timelines']
  timelines = json_data_arg['data']['timelines']
  intervals = []
  for timeline in timelines:
      intervals.extend(timeline['intervals'])
  
  # Create a DataFrame from the extracted data
  data = pd.DataFrame(intervals)
  
  
  # %% Turn Datetimes into columns
  df = pd.DataFrame(data['startTime'])
  
  # Split datetime into separate columns
  df[['date', 'time']] = df['startTime'].str.split('T', expand=True)
  df[['year', 'month', 'day']] = df['date'].str.split('-', expand=True)
  df[['hour', 'minute', 'second']] = df['time'].str[:-1].str.split(':', expand=True)
  
  # Remove unnecessary columns
  df = df.drop(['startTime', 'date', 'time'], axis=1)
  
  # %% Pull the column of temperature values
  df['temp'] = data['values'].apply(lambda x: x['temperature'])
      # access values in 'values' column
      # lambda fxn: extracts value, temp, from each key, x
      
  return(df)

# df2 = jsonstring_to_df(json_data)
df2 = jsonstring_to_df(string) # Attention: this works
```

## Addtl Fxns

```{python df to DB fxn -- not working}
df=df2

# PostgreSQL connection parameters
host = "localhost"
port = "5432"
database ="postgres"
user = "postgres"
password = "Brazilll"
schema = 'public'
table_name = 'Trial'

# conn_string = 'postgres://user:password@host/data1'
conn_string = database+'://'+user+':'+password+'@'+host+'/data1'
eng_string = database+'://'+user+':'+password+'@'+host+':'+port+'/'+database

engine = create_engine(eng_string)
# engine = create_engine('postgresql://username:password@localhost:5432/mydatabase')
df.to_sql('table_name', engine)

sql.write_frame(df, 'table_name', con, flavor='postgresql')
```

```{python Prints DataFrame to a table in the PostgreSQL database}
# Create a DataFrame
# df = pd.DataFrame({
#     'col1': [1, 2, 3],
#     'col2': ['A', 'B', 'C']
# })

# PostgreSQL connection parameters
host = "localhost"
port = "5432"
database ="postgres"
user = "postgres"
password = "Brazilll"
schema = 'public'
table_name = 'Trial'

# Create the SQLAlchemy engine
engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{database}')

# Convert the DataFrame to a table in the PostgreSQL database
df2.to_sql(table_name, engine, schema=schema, if_exists='replace', index=False)

# Close the database connection
engine.dispose()

# This method runs without error but not giving me any tables in DB
```

## Analysis

```{python match forecasts with historical data using dates}
# Should result in columns side by side

```

```{python find differences between historical and forecasted data}


```

# Summary statistics

```{python fxn for summary statistics and plots}
# stats for historical

# stats for forecasted

# stats for differences
```

## Saving it as DF as well for analytics?
